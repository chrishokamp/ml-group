{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv\n",
    "from theano.tensor.signal import downsample\n",
    "from theano import function\n",
    "import numpy\n",
    "import math\n",
    "from theano.ifelse import ifelse\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# theano.config.profile=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = numpy.zeros((2*4,3*4,1,3))\n",
    "# #print a, \"\\n\\n\"\n",
    "# for i in range(4):\n",
    "#     b = numpy.arange(i*18,18*(i+1)).reshape((2,3,1,3))\n",
    "#     a[i*2:(i+1)*2,i*3:(i+1)*3] = b\n",
    "# print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SharedZeros(shape):\n",
    "    zeros = numpy.zeros(shape, dtype=theano.config.floatX)\n",
    "    return theano.shared(zeros, borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConvLayer1D(object):\n",
    "    def __init__(self,rng, input, filter_shape, emb_size, activation=None, conv_border='full'):\n",
    "        \n",
    "        self.activation = activation\n",
    "        out_fmaps, in_fmaps, height, width = filter_shape\n",
    "    \n",
    "        assert height == 1\n",
    "        #\n",
    "        # V2: \n",
    "        #\n",
    "        # \n",
    "        self.input = input\n",
    "        \n",
    "        \n",
    "#         fan_in = in_fmaps * 1 * width\n",
    "#         fan_out = out_fmaps * 1 * width\n",
    "#         W_bound = numpy.sqrt(6./(fan_in+fan_out))\n",
    "\n",
    "        \n",
    "        w = numpy.asarray(\n",
    "                    numpy.random.normal(0, 0.05, \n",
    "                                size=(emb_size, out_fmaps, in_fmaps, 1, width)),\n",
    "                    dtype=theano.config.floatX\n",
    "                )\n",
    "#         w = numpy.ones((emb_size, out_fmaps, in_fmaps, 1, width),\n",
    "#                        dtype=theano.config.floatX)\n",
    "        self.W = theano.shared(w, borrow=True)\n",
    "        \n",
    "        #\n",
    "        # one conv for each time-line\n",
    "        #\n",
    "        conv_list = []\n",
    "        for i in range(emb_size):\n",
    "            # conv\n",
    "            conv_out = conv.conv2d(\n",
    "                input = self.input[:,:,i:i+1,:],\n",
    "                filters=self.W[i],\n",
    "                border_mode=conv_border\n",
    "            )\n",
    "            \n",
    "            conv_list.append(conv_out)\n",
    "        \n",
    "        conv_out  = T.concatenate(conv_list, axis=2)\n",
    "        \n",
    "\n",
    "        #self.b = theano.shared(numpy.asarray(rng.rand(emb_size), \n",
    "        #                             dtype=theano.config.floatX))\n",
    "        \n",
    "        linear_output = conv_out #+ self.b.dimshuffle('x','x',0,'x')\n",
    "        # use activation function\n",
    "        if self.activation is None:\n",
    "            self.output = linear_output\n",
    "        else:\n",
    "            self.output = self.activation(linear_output)\n",
    "        \n",
    "        # params\n",
    "        self.params = [self.W]\n",
    "        \n",
    "        # regularization\n",
    "        self.l1_reg = abs(self.W).sum()\n",
    "        self.l2_reg = (self.W**2).sum()\n",
    "        \n",
    "        #### gradient history for adagrad\n",
    "        self.Wgrad_hist = SharedZeros((emb_size, out_fmaps, in_fmaps, 1, width))\n",
    "        #self.bgrad_hist = SharedZeros((emb_size,))\n",
    "        \n",
    "        self.grad_hist = [self.Wgrad_hist]\n",
    "        \n",
    "#         self.output = conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input = T.tensor4()\n",
    "# rng=numpy.random\n",
    "# convlayer = ConvLayer1D(rng, input, filter_shape=(2,2,1,2), 2)\n",
    "# f = theano.function([input], convlayer.output)\n",
    "\n",
    "# a = numpy.asarray(range(16), dtype=theano.config.floatX).reshape((2,2,2,2))\n",
    "# a1 = f(a)\n",
    "\n",
    "# print a.shape\n",
    "# print a1.shape\n",
    "\n",
    "# # kvec=T.ivector()\n",
    "# # dkm = DynamicKMaxPoolLayer(input, kvec)\n",
    "# # f1 = theano.function([input, kvec], dkm.output)\n",
    "\n",
    "# # b = numpy.asarray([2, 3, 1, 4], dtype='int32')\n",
    "\n",
    "# # print f1(a1,b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dynamic k-max polling\n",
    "# pass a mask to take dynamic pooling into consideration\n",
    "class DynamicKMaxPoolLayer(object):\n",
    "    def __init__(self, input, kvec, kmax, mask=None):\n",
    "        # input is a 4D tensor\n",
    "        self.input = input\n",
    "        \n",
    "        if mask is None:\n",
    "            self.indices = T.argsort(input, axis=3)\n",
    "        else:\n",
    "            #assert mask.shape[0] == input.shape[0]\n",
    "            #assert mask.shape[1] == input.shape[2]\n",
    "            \n",
    "            #\n",
    "            # advanced indexing: not work on GPU\n",
    "            #\n",
    "            \n",
    "            # reshape to input shape\n",
    "            indexes = mask.reshape((mask.shape[0], 1, 1, mask.shape[1]))\n",
    "            # repeat along some axis\n",
    "            indexes = T.extra_ops.repeat(indexes,input.shape[2], axis=2)\n",
    "            indexes = T.extra_ops.repeat(indexes,input.shape[1], axis=1)\n",
    "            # index zeros\n",
    "            indexes = ( indexes< 1).nonzero()\n",
    "            # set to -inf\n",
    "            self.input = T.set_subtensor(self.input[indexes], float('-inf'))\n",
    "            # sort\n",
    "            self.indices = self.input.argsort(axis=3)\n",
    "            # set to zero\n",
    "            self.input = T.set_subtensor(self.input[indexes], float(0))\n",
    "        \n",
    "        #kmax = T.max(kvec)\n",
    "        # for each k, use scan\n",
    "        def step(input3d, indices3d, k, maxk):\n",
    "            topk_indices = indices3d[:,:,-k:]\n",
    "            ordered_topk_indices = topk_indices.sort(axis=2)\n",
    "            reshaped = input3d.reshape((input3d.shape[0]*input3d.shape[1],input3d.shape[2]))\n",
    "            output = reshaped[T.arange(reshaped.shape[0]).reshape((reshaped.shape[0],1)),\n",
    "                              ordered_topk_indices.reshape((reshaped.shape[0],k))]\n",
    "            output = output.reshape(ordered_topk_indices.shape)\n",
    "            # padding zero \n",
    "            zeros = T.zeros((input3d.shape[0], input3d.shape[1], T.cast(maxk-k,dtype='int32')), \n",
    "                            dtype=theano.config.floatX)\n",
    "            return T.concatenate([output, zeros], axis=2)\n",
    "            \n",
    "        results, _ = theano.scan(fn=step, \n",
    "                                 outputs_info=None,\n",
    "                                 sequences=[self.input, self.indices, kvec], \n",
    "                                 non_sequences=[kmax])\n",
    "        # output\n",
    "        self.output = T.concatenate([results], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input = T.tensor4()\n",
    "# kvec = T.ivector()\n",
    "# mask = T.imatrix()\n",
    "# dkm = DynamicKMaxPoolLayer(input, kvec, 4, mask=mask)\n",
    "\n",
    "# f = theano.function([input, kvec, mask], dkm.output)\n",
    "\n",
    "# a = numpy.arange(48, dtype=theano.config.floatX).reshape(2,2,3,4)\n",
    "# b = numpy.asarray([3,1], dtype='int32')\n",
    "# m = numpy.asarray([[1,1,1,0],[0,1,0,0]],dtype='int32')\n",
    "# print a\n",
    "# print f(a,b,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k-max polling\n",
    "# pass a mask to take dynamic pooling into consideration\n",
    "class KMaxPoolLayer(object):\n",
    "    def __init__(self, input, k, mask=None):\n",
    "        # input is a 4D tensor\n",
    "        self.input = input\n",
    "        \n",
    "        if mask is None:\n",
    "            self.indices = T.argsort(input, axis=3)\n",
    "        else:\n",
    "            #assert mask.shape[0] == input.shape[0]\n",
    "            #assert mask.shape[1] == input.shape[2]\n",
    "            \n",
    "            #\n",
    "            # advanced indexing: not work on GPU !!!\n",
    "            #\n",
    "            \n",
    "            # reshape to input shape\n",
    "            indexes = mask.reshape((mask.shape[0], 1, 1, mask.shape[1]))\n",
    "            # repeat along some axis\n",
    "            indexes = T.extra_ops.repeat(indexes,input.shape[2], axis=2)\n",
    "            indexes = T.extra_ops.repeat(indexes,input.shape[1], axis=1)\n",
    "            # index zeros\n",
    "            indexes = ( indexes< 1).nonzero()\n",
    "            # set to -inf\n",
    "            self.input = T.set_subtensor(self.input[indexes], float('-inf'))\n",
    "            # sort\n",
    "            self.indices = self.input.argsort(axis=3)\n",
    "            # set to zero\n",
    "            self.input = T.set_subtensor(self.input[indexes], float(0))\n",
    "            \n",
    "        # topk indices\n",
    "        topk_indices = self.indices[:,:,:,-k:]\n",
    "        # re-store the order\n",
    "        ordered_topk_indices = topk_indices.sort(axis=3)\n",
    "        \n",
    "        # this is one implemetation by reshape\n",
    "        # should consider more efficient ways\n",
    "        reshaped = self.input.reshape((T.prod(self.input.shape[:3]),self.input.shape[3]))\n",
    "        output = reshaped[T.arange(reshaped.shape[0]).reshape((reshaped.shape[0],1)),\n",
    "                          ordered_topk_indices.reshape((reshaped.shape[0],k))]\n",
    "        self.output = output.reshape(ordered_topk_indices.shape)\n",
    "#         self.oti = ordered_topk_indices\n",
    "#         self.ti = topk_indices\n",
    "#         self.get_indices = function([input, k], self.oti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input = T.tensor4()\n",
    "# mask = T.imatrix()\n",
    "\n",
    "# dklayer = KMaxPoolLayer(input, 3, mask)\n",
    "# f = theano.function([input, mask], dklayer.output)\n",
    "\n",
    "# a = numpy.asarray(range(32), dtype=theano.config.floatX).reshape((2,2,2,4))\n",
    "# # numpy.random.shuffle(a)\n",
    "# b = numpy.asarray([1,1,0,0,1,0,1,0], dtype='int32').reshape(2,4)\n",
    "\n",
    "# out = f(a,b)\n",
    "# print a\n",
    "# print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FoldingLayer(object):\n",
    "    def __init__(self, input, fold_len=2):\n",
    "        shape = input.shape\n",
    "        extra = shape[2]%fold_len\n",
    "        input = ifelse(T.eq(extra, 0), \n",
    "                       input, \n",
    "                       T.concatenate([input, \n",
    "                                     T.zeros((shape[0], shape[1], fold_len-extra, shape[3]),\n",
    "                                             dtype=theano.config.floatX)],\n",
    "                                     axis=2))\n",
    "        \n",
    "        new_len = T.cast(T.ceil(shape[2]*1.0/fold_len), 'int32')\n",
    "        X = input.reshape((shape[0], shape[1], new_len, fold_len, shape[3]))\n",
    "        X = T.sum(X, axis=3)\n",
    "        X = X.reshape((shape[0],shape[1],new_len,shape[3]))\n",
    "        self.output = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = numpy.asarray(range(24), dtype=theano.config.floatX).reshape((2,2,3,2))\n",
    "# input = T.tensor4()\n",
    "# fold = FoldingLayer(input)\n",
    "# f = theano.function([input],fold.output)\n",
    "# print a\n",
    "# print f(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NonLinearLayer(object):\n",
    "    def __init__(self, rng, input, size, fmaps,  activation=T.tanh):\n",
    "        self.activation = activation\n",
    "        self.b = theano.shared(numpy.asarray(numpy.random.rand(fmaps, size), dtype=theano.config.floatX))\n",
    "        self.output = self.activation(input + self.b.dimshuffle('x',0,1,'x'))\n",
    "        self.params = [self.b]\n",
    "\n",
    "        self.bgrad_hist = SharedZeros((fmaps, size))\n",
    "        self.grad_hist = [self.bgrad_hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = numpy.asarray(range(32), dtype=theano.config.floatX).reshape((2,2,4,2))\n",
    "# input = T.tensor4()\n",
    "# nl = NonLinearLayer(input, 4)\n",
    "# f = theano.function([input],nl.output)\n",
    "# print a\n",
    "# print f(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DropoutLayer(object):\n",
    "    def __init__(self, rng, input, p):\n",
    "        \"\"\"p is the probablity of dropping a unit\n",
    "        \"\"\"\n",
    "        srng = theano.tensor.shared_randomstreams.RandomStreams(numpy.random.randint(999999))\n",
    "        # p=1-p because 1's indicate keep and p is prob of dropping\n",
    "        mask = srng.binomial(n=1, p=1-p, size=input.shape)\n",
    "        # The cast is important because\n",
    "        # int * float32 = float64 which pulls things off the gpu\n",
    "        self.output = input * T.cast(mask, theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = numpy.asarray(range(32), dtype=theano.config.floatX).reshape((2,2,4,2))\n",
    "# input = T.tensor4()\n",
    "# drop = DropoutLayer(None, input, 0.5)\n",
    "# f = theano.function([input],drop.output)\n",
    "# print a\n",
    "# print f(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: input reshape\n",
    "# TODO: define error, params, and functions\n",
    "class ConvSentModel (object):\n",
    "    # so many parameters !!!\n",
    "    def __init__(self, rng, input, sentlen, mask, vocab_size=1000, num_feats=48, max_len=58, \n",
    "                 embedding=None, m1=7, m2=5, fmap1=6, fmap2=14, topk=4, \n",
    "                 num_class=2, drop_prob=0.5, fold_len=2):\n",
    "        \n",
    "        ##rng = numpy.random.RandomState(1234)\n",
    "        \n",
    "        self.input = input\n",
    "        self.sentlen = sentlen\n",
    "        self.mask = mask\n",
    "        \n",
    "        # input variable\n",
    "        #input = T.imatrix()\n",
    "        #sentlen = T.ivector()\n",
    "        \n",
    "        # get batch size\n",
    "        batch_size = self.input.shape[0]\n",
    "        \n",
    "        # wordvec parameter\n",
    "        if embedding is None:\n",
    "            w_bound = numpy.sqrt(6./(vocab_size+num_feats))\n",
    "            self.embedding = theano.shared(numpy.asarray(\n",
    "                    rng.normal(0, 0.05,\n",
    "                               size=(num_feats,vocab_size)), \n",
    "                    dtype=theano.config.floatX))\n",
    "        else:\n",
    "            self.embedding = embedding\n",
    "        # add an extrac row at last for padding short sentences\n",
    "        emb_ext = T.concatenate([self.embedding, T.zeros((num_feats,1), \n",
    "                                                         dtype=theano.config.floatX)], \n",
    "                                axis=1)\n",
    "        \n",
    "        # get input matrix for batched sentences\n",
    "        # resshape the matrix for 1D convolution layer input\n",
    "        # (batch_size, input_feature_maps, num_feats, max_sent_len)\n",
    "        in_matrix = emb_ext.transpose()[self.input].reshape(\n",
    "            (batch_size, 1, max_len, num_feats)).dimshuffle(0,1,3,2)\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        ### the first part is : wide_conv + folding + dynamic k-max + non-linear\n",
    "        ###\n",
    "        \n",
    "        # wide convolution\n",
    "        #conv1_output_shape = (batch_size, fmap1, num_feat, max_len+m1-1)\n",
    "        conv1 = ConvLayer1D(rng, in_matrix, \n",
    "                            filter_shape=(fmap1, 1, 1, m1),\n",
    "                            emb_size=num_feats,\n",
    "                            activation=None)\n",
    "        # folding\n",
    "        fold1 = FoldingLayer(conv1.output, fold_len=fold_len)\n",
    "        \n",
    "        # now the num_feats has become to math.ceil(num_features/2)\n",
    "        num_feats1 = num_feats/fold_len\n",
    "#         num_feats1 = num_feats\n",
    "        # dynamic k-max\n",
    "        # depends on the length of input and it's depth in the network\n",
    "        # the first dynamic-kmax-pooling\n",
    "        # zero are padded if < max-k\n",
    "        max_dk = int(math.ceil(0.5 * max_len))\n",
    "        total_len1 = max_len+m1-1\n",
    "        # a mask is need to enable variable sent-length\n",
    "        # define mask-step which create a mask-vec for one sent\n",
    "#         def maskstep1(slen):\n",
    "#             mask_vec = T.zeros((total_len1,))\n",
    "#             mask_vec = T.set_subtensor(mask_vec[:slen+m1-1], 1)\n",
    "#             return mask_vec.transpose(), T.cast(T.max([T.ceil(0.5 * slen), topk]), dtype='int32')\n",
    "#         # the max_len after convolution\n",
    "        \n",
    "#         [mask_vecs1, dkvec1], _ = theano.scan(fn=maskstep1, \n",
    "#                                  outputs_info=None,\n",
    "#                                  sequences=[self.sentlen])\n",
    "# #         concatenate together to get a matrix\n",
    "#         mask1 = T.concatenate([mask_vecs1], axis=0)\n",
    "        # create a dkmax-pool layer\n",
    "        real_dkvec1 = T.cast(T.ceil(0.5 * self.sentlen), dtype='int32')\n",
    "        dkvec1 = T.set_subtensor(real_dkvec1[(real_dkvec1<topk).nonzero()], topk)\n",
    "        mask1 = self.mask[:, :total_len1]\n",
    "        dkmax = DynamicKMaxPoolLayer(fold1.output, dkvec1, max_dk, mask=mask1)\n",
    "        \n",
    "        # apply a non-linear function\n",
    "        nonlinear1 = NonLinearLayer(rng, dkmax.output, num_feats1, fmap1)\n",
    "        \n",
    "        ###\n",
    "        ### second part is : wide_conv + folding + k-max + non-linear\n",
    "        ###\n",
    "        \n",
    "        # wide convolution\n",
    "        conv2 = ConvLayer1D(rng, nonlinear1.output, \n",
    "                            filter_shape=(fmap2, fmap1, 1, m2), \n",
    "                            emb_size=num_feats1,\n",
    "                            activation=None)\n",
    "        # folding\n",
    "        fold2 = FoldingLayer(conv2.output, fold_len=fold_len)\n",
    "        num_feats2 = num_feats1/fold_len\n",
    "        \n",
    "        # k-max\n",
    "        # also need a mask\n",
    "        total_len2 = max_dk + m2 - 1\n",
    "#         def maskstep2(real_dk):\n",
    "#             mask_vec = T.zeros((total_len2,))\n",
    "#             mask_vec = T.set_subtensor(mask_vec[:real_dk+m2-1], 1)\n",
    "#             return mask_vec.transpose()\n",
    "        \n",
    "#         mask_vecs2, _ = theano.scan(fn=maskstep2, \n",
    "#                                     outputs_info=None,\n",
    "#                                     sequences=[dkvec1])\n",
    "#         mask2 = T.concatenate([mask_vecs2], axis=0)\n",
    "        # create a kmaxlayer\n",
    "        mask2 = self.mask[:, total_len1:]\n",
    "        kmax = KMaxPoolLayer(fold2.output, topk, mask=mask2)\n",
    "        \n",
    "        #apply another non-linear\n",
    "        nonlinear2 = NonLinearLayer(rng, kmax.output, num_feats2, fmap2)\n",
    "        \n",
    "        # flattern the output to get sentence vec\n",
    "        sent_dim = topk * num_feats2 * fmap2\n",
    "        sentvecs = nonlinear2.output.reshape((batch_size,sent_dim))\n",
    "        \n",
    "        ###\n",
    "        ### before the last part: apply a dropoutlayer\n",
    "        ###\n",
    "        \n",
    "        # not used now\n",
    "        dropout = DropoutLayer(rng, sentvecs, drop_prob)\n",
    "        \n",
    "        ###\n",
    "        ### the third part, also the last part, is a fully connected layer\n",
    "        ###\n",
    "        \n",
    "        w_bound = numpy.sqrt(6./(num_class+sent_dim))\n",
    "        self.W_out = theano.shared(\n",
    "            numpy.asarray(\n",
    "                numpy.random.normal(0, 0.05,\n",
    "                           size=(sent_dim,num_class)),\n",
    "                dtype=theano.config.floatX))\n",
    "        self.b_out = theano.shared(numpy.asarray(\n",
    "                numpy.random.rand(num_class), \n",
    "                dtype=theano.config.floatX))\n",
    "        \n",
    "        #softmax output\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(dropout.output, self.W_out) + self.b_out)\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        \n",
    "        #softmax output for test, without dropout\n",
    "        self.p_y_given_x_test = T.nnet.softmax(T.dot(sentvecs, self.W_out) + self.b_out)\n",
    "        self.y_pred_test = T.argmax(self.p_y_given_x_test, axis=1)\n",
    "        \n",
    "        \n",
    "        # some useful attributs\n",
    "        self.params = ( conv1.params + nonlinear1.params \n",
    "                       + conv2.params + nonlinear2.params \n",
    "                       + [self.embedding, self.W_out, self.b_out]\n",
    "                       )\n",
    "        \n",
    "        self.l1_reg = (conv1.l1_reg + conv2.l1_reg \n",
    "                       + abs(self.W_out).sum() \n",
    "                       + abs(self.embedding).sum())\n",
    "        self.l2_reg = (conv1.l2_reg + conv2.l2_reg \n",
    "                       + (self.W_out ** 2).sum() \n",
    "                       + (self.embedding**2).sum())\n",
    "        \n",
    "        # gradient history for adagrad\n",
    "        self.embgrad_hist = SharedZeros((num_feats,vocab_size))\n",
    "        self.Wgrad_hist = SharedZeros((sent_dim,num_class))\n",
    "        self.bgrad_hist = SharedZeros((num_class,))\n",
    "        # the same order with params\n",
    "        self.grad_hist = (conv1.grad_hist + nonlinear1.grad_hist\n",
    "                         + conv2.grad_hist + nonlinear2.grad_hist\n",
    "                         + [self.embgrad_hist, self.Wgrad_hist, self.bgrad_hist])\n",
    "    \n",
    "    def negative_log_likelihood(self,y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "    \n",
    "    def errors(self,y):\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred_test.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred_test.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred_test, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "#         # gradients\n",
    "#         gparams = T.grad(self.cost, self.params)\n",
    "#         updates = []\n",
    "#         for param, gparam in zip(self.params, gparams):\n",
    "#             updates.append([param, param - learning_rate * gparam])\n",
    "        \n",
    "#         # tools: some useful functions\n",
    "#         self.Train = theano.function([input, sentlen, y], self.cost, updates=updates)\n",
    "#         self.SentenceVector = theano.function([input, sentlen], sentvecs)\n",
    "#         self.Predict = theano.function([input, sentlen], self.y_pred)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ConvSentModel(100, 10, 20)\n",
    "# rng = numpy.random.RandomState(1234)\n",
    "# x = T.imatrix('x')  # the data is presented as rasterized images\n",
    "# s = T.ivector('s') # sentence length\n",
    "# ConvSentModel(rng, x,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mask = T.zeros((3,5))\n",
    "# sent = ivector()\n",
    "# mask1 = T.set_subtensor(mask[T.arange(sent.shape[1]), ], 1)\n",
    "# f = theano.function([sent], mask1)\n",
    "# m = f([1,3,4])\n",
    "# print m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "### read data to test\n",
    "###\n",
    "\n",
    "def load_sst_data(emb_size=48):\n",
    "    ''' Loads the dataset\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path to the dataset (here MNIST)\n",
    "    '''\n",
    "\n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    "    matFile = 'sst/SST_binary.mat'\n",
    "    import scipy.io\n",
    "    data = scipy.io.loadmat(matFile)\n",
    "    \n",
    "#     print '... loading data'\n",
    "\n",
    "#     # Load the dataset\n",
    "#     # keys: \n",
    "#     data = scipy.io.loadmat(matFile)\n",
    "    \n",
    "#     pklFile = 'sst/SST_dep.pkl'\n",
    "#     import cPickle\n",
    "#     data = cPickle.load(open(pklFile, 'rb'))\n",
    "    \n",
    "    train_set = data['train']\n",
    "    train_lbl_set = data['train_lbl']\n",
    "    valid_set = data['valid']\n",
    "    valid_lbl_set = data['valid_lbl']\n",
    "    test_set = data['test']\n",
    "    test_lbl_set = data['test_lbl']\n",
    "    emb = data['vocab_emb'][:emb_size]\n",
    "    \n",
    "    embedding = theano.shared(numpy.asarray(emb, \n",
    "                                            dtype=theano.config.floatX),\n",
    "                             borrow=True)\n",
    "    \n",
    "    #train_set, valid_set, test_set format: tuple(input, target)\n",
    "    #input is an numpy.ndarray of 2 dimensions (a matrix)\n",
    "    #witch row's correspond to an example. target is a\n",
    "    #numpy.ndarray of 1 dimensions (vector)) that have the same length as\n",
    "    #the number of rows in the input. It should give the target\n",
    "    #target to the example with the same index in the input.\n",
    "\n",
    "    def shared_dataset(data_x, data_ys, borrow=True):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        data_y, data_s = data_ys[:,0], data_ys[:,1]\n",
    "        shared_x = theano.shared(numpy.asarray(data_x-1,\n",
    "                                               dtype='int32'),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(numpy.asarray(data_y-1,\n",
    "                                               dtype='int32'),\n",
    "                                 borrow=borrow)\n",
    "        shared_s = theano.shared(numpy.asarray(data_s,\n",
    "                                               dtype='int32'),\n",
    "                                 borrow=borrow)\n",
    "        # When storing data on the GPU it has to be stored as floats\n",
    "        # therefore we will store the labels as ``floatX`` as well\n",
    "        # (``shared_y`` does exactly that). But during our computations\n",
    "        # we need them as ints (we use labels as index, and if they are\n",
    "        # floats it doesn't make sense) therefore instead of returning\n",
    "        # ``shared_y`` we will have to cast it to int. This little hack\n",
    "        # lets ous get around this issue\n",
    "        return shared_x, shared_y, shared_s\n",
    "\n",
    "    test_set_x, test_set_y, test_set_s = shared_dataset(test_set, test_lbl_set)\n",
    "    valid_set_x, valid_set_y, valid_set_s = shared_dataset(valid_set, valid_lbl_set)\n",
    "    train_set_x, train_set_y, train_set_s = shared_dataset(train_set, train_lbl_set)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y, train_set_s), \n",
    "            (valid_set_x, valid_set_y, valid_set_s),\n",
    "            (test_set_x, test_set_y, test_set_s), embedding]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# matFile = 'sst/SST_binary.mat'\n",
    "# import scipy.io\n",
    "# data = scipy.io.loadmat(matFile)\n",
    "\n",
    "# print len(data['index'])\n",
    "# #for i in range(len(data['index'])):\n",
    "# print data['index'][65]\n",
    "# x = [data['index'][i,0] for i in xrange(len(data['index']))]\n",
    "# y = [i[0] if len(i) > 0 else '' for i in x]\n",
    "# print [i for i in y]\n",
    "# print '... loading data'\n",
    "\n",
    "# data = scipy.io.loadmat(matFile)\n",
    "\n",
    "# print sum((data['train_lbl'][:,0]-1)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "two masks: one for dkmax, one for kmax\n",
    "concatenated\n",
    "'''\n",
    "\n",
    "def create_mask(S, max_len, topk, m1, m2):\n",
    "    size = S.shape[0]\n",
    "    l1 = max_len+m1-1\n",
    "    max_dk = int(math.ceil(max_len*0.5))\n",
    "    l2 = max_dk + m2-1\n",
    "    mask = numpy.zeros((size, l1+l2),\n",
    "                      dtype='int32')\n",
    "    for i in range(size):\n",
    "        real_m1 = S[i] + m1-1\n",
    "        mask[i, :real_m1] = 1\n",
    "        real_dk = max([topk, int(math.ceil(0.5*S[i]))])\n",
    "        real_m2 = real_dk + m2-1\n",
    "        mask[i, l1:l1+real_m2] = 1\n",
    "    M = theano.shared(mask, borrow=True)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_DCNN(batch_size = 5, l1_rate=0, l2_rate=0.0001, learning_rate=0.1,\n",
    "             n_epochs=20):\n",
    "    \"\"\"\n",
    "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
    "    perceptron\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "    gradient\n",
    "\n",
    "    :type L1_reg: float\n",
    "    :param L1_reg: L1-norm's weight when added to the cost (see\n",
    "    regularization)\n",
    "\n",
    "    :type L2_reg: float\n",
    "    :param L2_reg: L2-norm's weight when added to the cost (see\n",
    "    regularization)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path of the MNIST dataset file from\n",
    "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "\n",
    "\n",
    "   \"\"\"\n",
    "    \n",
    "    emb_size = 8\n",
    "    \n",
    "    datasets = load_sst_data(emb_size)\n",
    "\n",
    "    train_set_x, train_set_y, train_set_s = datasets[0]\n",
    "    valid_set_x, valid_set_y, valid_set_s = datasets[1]\n",
    "    test_set_x, test_set_y, test_set_s = datasets[2]\n",
    "    embedding = datasets[3]\n",
    "    #vocab_size = datasets[4]\n",
    "    #sent_length = datasets[5]\n",
    "    \n",
    "    #emb_size = 48 #embedding.get_value(borrow=True).shape[0]\n",
    "    vocab_size = embedding.get_value(borrow=True).shape[1]\n",
    "    sent_length = train_set_x.get_value(borrow=True).shape[1]\n",
    "    \n",
    "    print 'emb_size: ', emb_size\n",
    "    print 'vocab_size: ', vocab_size\n",
    "    print 'max_sent_len: ', sent_length\n",
    "    \n",
    "    \n",
    "    # create mask\n",
    "    m1=7\n",
    "    m2=5\n",
    "    topk=4\n",
    "    \n",
    "    train_set_m = create_mask(train_set_s.get_value(borrow=True),\n",
    "                             sent_length, topk, m1, m2)\n",
    "    valid_set_m = create_mask(valid_set_s.get_value(borrow=True),\n",
    "                             sent_length, topk, m1, m2)\n",
    "    test_set_m = create_mask(test_set_s.get_value(borrow=True),\n",
    "                             sent_length, topk, m1, m2)\n",
    "    \n",
    "#     print train_set_m.get_value(borrow=True)\n",
    "    # end crete mask\n",
    "    \n",
    "    print (train_set_x.get_value(borrow=True).shape[0],\n",
    "           valid_set_x.get_value(borrow=True).shape[0],\n",
    "           test_set_x.get_value(borrow=True).shape[0])\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print '... building the model'\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.imatrix('x')  # the data is presented as rasterized images\n",
    "    s = T.ivector('s') # sentence length\n",
    "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                        # [int] labels\n",
    "    m = T.imatrix('m') # mask\n",
    "\n",
    "    rng = numpy.random.RandomState(123456789)\n",
    "\n",
    "    # construct the MLP class\n",
    "    classifier = ConvSentModel(\n",
    "        rng=rng,\n",
    "        input=x,\n",
    "        sentlen=s,\n",
    "        mask=m,\n",
    "        vocab_size = vocab_size,\n",
    "        num_feats=emb_size,\n",
    "        max_len=sent_length,\n",
    "        embedding=embedding,\n",
    "        topk=topk, m1=m1, m2=m2\n",
    "    )\n",
    "\n",
    "    # start-snippet-4\n",
    "    # the cost we minimize during training is the negative log likelihood of\n",
    "    # the model plus the regularization terms (L1 and L2); cost is expressed\n",
    "    # here symbolically\n",
    "    \n",
    "    cost = (\n",
    "        classifier.negative_log_likelihood(y)\n",
    "        + l1_rate * classifier.l1_reg\n",
    "        + l2_rate * classifier.l2_reg\n",
    "    )\n",
    "    # end-snippet-4\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made\n",
    "    # by the model on a minibatch\n",
    "    test_model = theano.function(\n",
    "        on_unused_input='warn',\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index:],\n",
    "            s: test_set_s[index:],\n",
    "            y: test_set_y[index:],\n",
    "            m: test_set_m[index:]\n",
    "        },\n",
    "        mode='FAST_RUN'\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        on_unused_input='warn',\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index:],\n",
    "            s: valid_set_s[index:],\n",
    "            y: valid_set_y[index:],\n",
    "            m: valid_set_m[index:]\n",
    "        },\n",
    "        mode='FAST_RUN'\n",
    "    )\n",
    "    \n",
    "    # start-snippet-5\n",
    "    # compute the gradient of cost with respect to theta (sotred in params)\n",
    "    # the resulting gradients will be stored in a list gparams\n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs\n",
    "\n",
    "    # given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of\n",
    "    # same length, zip generates a list C of same size, where each element\n",
    "    # is a pair formed from the two lists :\n",
    "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
    "    \n",
    "#     updates = [\n",
    "#         (param, param - sq * gparam)\n",
    "#         for param, gparam in zip(classifier.params, gparams)\n",
    "#     ]\n",
    "    \n",
    "    # adagrad update\n",
    "    updates = []\n",
    "    for param, gparam, grad_hist in zip(classifier.params, gparams, classifier.grad_hist):\n",
    "        gh = grad_hist + gparam ** 2\n",
    "        sq = T.sqrt(gh)\n",
    "        sq = T.set_subtensor(sq[sq.nonzero()], learning_rate / sq[sq.nonzero()])\n",
    "        \n",
    "        updates.append((param, param - sq * gparam))\n",
    "        updates.append((grad_hist, grad_hist + gparam ** 2))\n",
    "    \n",
    "#     updates.append((grad_hist, grad_hist + gparams.flatten() ** 2))\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but\n",
    "    # in the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        on_unused_input='warn',\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            s: train_set_s[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size],\n",
    "            m: train_set_m[index * batch_size: (index + 1) * batch_size]\n",
    "        },\n",
    "        mode='FAST_RUN'\n",
    "    )\n",
    "    # end-snippet-5\n",
    "    \n",
    "    # reset grad_hist\n",
    "    xx = T.scalar()\n",
    "    update_reset = [(grad_hist, T.set_subtensor(grad_hist[:], xx)) for grad_hist in classifier.grad_hist]\n",
    "    reset_hist = theano.function([xx],updates=update_reset)\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print '... training'\n",
    "\n",
    "    # early-stopping parameters\n",
    "    patience = 10000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = 10#min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = time.clock()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        reset_hist(0.)\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            \n",
    "            #print classifier.grad_hist[-2].get_value(borrow=True)[0,:5]\n",
    "            #print \"gp_weig : \", classifier.params[-1].get_value(borrow=True)\n",
    "            #print \"gp_valu : \", gp\n",
    "            #print \"gp_hist : \", classifier.grad_hist[-1].get_value(borrow=True)\n",
    "            \n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            #if (iter + 1) % validation_frequency == 0:\n",
    "            if   (minibatch_index+1) %  validation_frequency == 0 or minibatch_index+1 == n_train_batches:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = validate_model(0)\n",
    "                this_validation_loss = validation_losses\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, PPL %f, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        math.exp(minibatch_avg_cost),\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                total_size = 0\n",
    "                total_value = 0.\n",
    "                for param in classifier.params:\n",
    "                    value = param.get_value(borrow=True)\n",
    "                    total_size += numpy.prod(value.shape)\n",
    "                    total_value += abs(value).sum()\n",
    "                avgw = total_value/total_size\n",
    "                print \"average weights %f\" % (avgw)\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (\n",
    "                        this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold\n",
    "                    ):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model(0)\n",
    "                    test_score = test_losses\n",
    "\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "#             if patience <= iter:\n",
    "#                 done_looping = True\n",
    "#                 break\n",
    "\n",
    "    end_time = time.clock()\n",
    "    print(('Optimization complete. Best validation score of %f %% '\n",
    "           'obtained at iteration %i, with test performance %f %%') %\n",
    "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print >> sys.stderr, ('The code ran for %.2fm' % ((end_time - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = numpy.arange(20).reshape(4,5)\n",
    "# b = numpy.asarray([[3,4],[1,2]])\n",
    "# print a\n",
    "# print a.transpose()[b].transpose([0,2,1]).reshape((2,1,4,2))\n",
    "# print a.transpose()[b].reshape((2,1,2,4)).transpose([0,1,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_size:  8\n",
      "vocab_size:  15447\n",
      "max_sent_len:  58\n",
      "(100, 872, 1821)"
     ]
    }
   ],
   "source": [
    "#theano.config.exception_verbosity='high'\n",
    "#theano.config.optimizer='None'\n",
    "if __name__ == '__main__':\n",
    "    test_DCNN(batch_size = 2, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
